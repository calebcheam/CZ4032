{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "MAX_LENGTH = 10\n",
    "def preprocess_str(review):\n",
    "    '''\n",
    "    Function to preprocess one string into list of tokens\n",
    "    '''\n",
    "    review = review.lower()\n",
    "    review = review.translate(str.maketrans(\"\", \"\",string.punctuation))\n",
    "    review = review.translate(str.maketrans('', '', string.digits))\n",
    "    review = review.strip()\n",
    "    \n",
    "    tokens = word_tokenize(review)\n",
    "    custom_stop_words =['just', 'wasnt', 'didnt', 'went', 'came',]\n",
    "    stop_words = frozenset().union(_stop_words.ENGLISH_STOP_WORDS, custom_stop_words)\n",
    "    tokens = [i for i in tokens if not i in stop_words]\n",
    "    tokens = [word for word in tokens if len(word)>=3]\n",
    "    \n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    # Extract adjectives from the tagged tokens\n",
    "    adjectives = [word for word, tag in tagged_tokens if tag == 'JJ']\n",
    "    \n",
    "    lemma = WordNetLemmatizer()\n",
    "    tokens = [lemma.lemmatize(word, pos=\"a\") for word in adjectives]\n",
    "    \n",
    "    res = list(set(tokens))\n",
    "    res.sort()\n",
    "    return res[:MAX_LENGTH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent\n",
    "from itertools import islice\n",
    "from collections import defaultdict\n",
    "import multiprocessing\n",
    "# from mlxtend.preprocessing import TransactionEncoder\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from ast import literal_eval\n",
    "\n",
    "NUM_CHUNKS = 0\n",
    "CHUNK_SIZE = 70_000\n",
    "\n",
    "def get_chunk_iter_raw(filepath=None):\n",
    "    if filepath is None:\n",
    "        filepath = \"yelp_academic_dataset_review.csv\"\n",
    "    chunk_iter = pd.read_csv(filepath, chunksize=CHUNK_SIZE)\n",
    "    if NUM_CHUNKS>0:\n",
    "        selected_chunks = list(islice(chunk_iter, NUM_CHUNKS))\n",
    "        chunk_iter = selected_chunks\n",
    "    return chunk_iter \n",
    "\n",
    "def for_csv(column_value):\n",
    "    if column_value is None:\n",
    "        print(column_value)\n",
    "        return \"[',']\"\n",
    "    \n",
    "    if len(column_value) == 1:\n",
    "        column_value.append(\"\")\n",
    "        \n",
    "    return \"['\" + \"','\".join(column_value) + \"']\"\n",
    "    \n",
    "def chunk__save_preprocessed(idx, chunk, skip=True):\n",
    "    '''\n",
    "    Preprocesses the text column into lists of list of word tokens\n",
    "    Then writes this to a csv. \n",
    "    (All the CSVs will be combined in the outer function)\n",
    "    '''\n",
    "    output_file = f'yelp_temp_{idx}.csv'\n",
    "    \n",
    "    if skip and os.path.isfile(output_file):\n",
    "        # skip preprocessing if already done before\n",
    "        print(f\"skipping pre-processing for chunk {idx}\")\n",
    "        return len(get_list_of_lists(idx)) \n",
    "    \n",
    "    list_of_lists = Parallel(n_jobs=4)(delayed(preprocess_str)(line) for line in chunk['text'])\n",
    "    list_of_lists = [l for l in list_of_lists if l]\n",
    "    df = pd.DataFrame({'text':list_of_lists})\n",
    "    df = df.dropna()\n",
    "    df['text'] = df['text'].apply(for_csv)\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"completed pre-processing for chunk {idx}\")\n",
    "    return len(list_of_lists)\n",
    "\n",
    "def process_all():\n",
    "    '''\n",
    "    Preprocessing function to be called once before apriori algorithm\n",
    "    '''\n",
    "    chunk_iter = get_chunk_iter_raw()\n",
    "    dataset_length = 0\n",
    "    chunk_iter_len = len(list(get_chunk_iter_raw()))\n",
    "    print(\"------------------PREPROCESSING ------------------\")\n",
    "    # for idx, chunk in enumerate(chunk_iter):\n",
    "    #     dataset_length+= chunk__save_preprocessed(idx, chunk)\n",
    "        \n",
    "    num_threads = multiprocessing.cpu_count()\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        chunk_lengths = list(executor.map(chunk__save_preprocessed,\n",
    "                                    range(chunk_iter_len), \n",
    "                                    chunk_iter), \n",
    "                       )\n",
    "\n",
    "    dataset_length = sum(chunk_lengths)\n",
    "    # combine_files(chunk_iter_len)\n",
    "    return dataset_length, chunk_iter_len\n",
    "    \n",
    "def combine_files(chunk_iter_len):\n",
    "    output_file_path = 'joined_file.csv'\n",
    "\n",
    "    with open(output_file_path, 'w', newline='') as output_file:\n",
    "        output_file.truncate() # ensure it is empty file\n",
    "        csv_writer = csv.writer(output_file)\n",
    "        \n",
    "        csv_files = [f'yelp_temp_{i}.csv' for i in range(chunk_iter_len)]\n",
    "\n",
    "        for csv_idx, csv_file in enumerate(csv_files):\n",
    "            print(csv_idx)\n",
    "            \n",
    "            with open(csv_file, 'r') as input_file:\n",
    "                csv_reader = csv.reader(input_file)\n",
    "                for row_idx, row in enumerate(csv_reader):\n",
    "                    if (csv_idx>0) and (row_idx==0): \n",
    "                        pass # not first csv file, but first row \n",
    "                    else:\n",
    "                        csv_writer.writerow(row)\n",
    "\n",
    " \n",
    "def get_list_of_lists(idx, chunk=None):\n",
    "    if chunk is None: \n",
    "        df = pd.read_csv(f\"yelp_temp_{idx}.csv\", converters={'text': literal_eval})\n",
    "        return df.text.to_list()\n",
    "    else: \n",
    "        return chunk['text'].apply(literal_eval).tolist()\n",
    "\n",
    "def get_chunk_iter_processed():\n",
    "    chunk_iter = pd.read_csv(\"joined_file.csv\", chunksize=CHUNK_SIZE)\n",
    "    if NUM_CHUNKS>0:\n",
    "        selected_chunks = list(islice(chunk_iter, NUM_CHUNKS))\n",
    "        chunk_iter = selected_chunks\n",
    "    return chunk_iter        \n",
    "               \n",
    "def chunk__get_all_unique_items(idx, chunk=None):\n",
    "    chunk_len_1_itemsets = defaultdict(int)\n",
    "    start = time.time()\n",
    "    list_of_lists = get_list_of_lists(idx, chunk)\n",
    "    # if idx%5==0: \n",
    "    #     print (f'retrieved chunk {idx}:\\t {round(time.time() - start,2)}')\n",
    "    \n",
    "    start = time.time()\n",
    "    for row in list_of_lists:   \n",
    "        for item in sorted(set(row)):  \n",
    "            chunk_len_1_itemsets[item] +=1 # if new item, default = 0 \n",
    "\n",
    "    if idx%10==0:\n",
    "        print(f'{len(chunk_len_1_itemsets)} unique items found in chunk {idx}. Took {round(time.time() - start,2)}')\n",
    "    return chunk_len_1_itemsets\n",
    "\n",
    "def combine_dictionaries(dicts):\n",
    "    combined_dict = defaultdict(list)\n",
    "    for my_dict in dicts:\n",
    "        for key, value in my_dict.items():\n",
    "            combined_dict[key].append(value)\n",
    "\n",
    "    print(f\"{len(combined_dict)} unique items\")\n",
    "    return {key: sum(values) for key, values in combined_dict.items()}\n",
    "        \n",
    "\n",
    "def get_all_unique_items(chunk_iter_len, chunk_iter = None):\n",
    "    '''\n",
    "    This function called only for the first run. \n",
    "    Scan database once to get all unique items \n",
    "    And also to count the support for these items \n",
    "    '''\n",
    "\n",
    "    num_threads = multiprocessing.cpu_count()\n",
    "    print(\"------------------FIRST RUN------------------\")\n",
    "    \n",
    "    if chunk_iter is None: \n",
    "        with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "\n",
    "            results = list(executor.map(chunk__get_all_unique_items,\n",
    "                                        range(chunk_iter_len), \n",
    "                                        ), \n",
    "                        )\n",
    "    else : \n",
    "        with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "\n",
    "            results = list(executor.map(chunk__get_all_unique_items,\n",
    "                                        range(chunk_iter_len), \n",
    "                                        chunk_iter\n",
    "                                        ), \n",
    "                        )     \n",
    "    \n",
    "    combined_dict = combine_dictionaries(results)\n",
    "    \n",
    "    return combined_dict\n",
    "    \n",
    "\n",
    "    # for idx, chunk in enumerate(data_chunks): \n",
    "    #     start = time.time()\n",
    "    #     list_of_lists = Parallel(n_jobs=4)(delayed(preprocess_str)(line) for line in chunk['text'])\n",
    "    #     print (f'chunk {idx+1}:\\t', round(time.time() - start,2))\n",
    "        \n",
    "    #     # list_of_lists = [row.dropna().tolist() for index, row in chunk.iterrows()]\n",
    "    #     for row in list_of_lists:\n",
    "    #         existing = set(len_1_itemsets.keys())\n",
    "    #         new_items = set(row).difference(existing)\n",
    "    #         initial_counts = np.zeros(len(new_items))\n",
    "    #         len_1_itemsets.update(dict(zip(list(new_items), initial_counts)))\n",
    "\n",
    "    #         reappeared = set(row).intersection(existing)\n",
    "    #         len_1_itemsets.update({item: len_1_itemsets[item] + 1 for item in reappeared})\n",
    "    #         dataset_length += 1 \n",
    "\n",
    "    # return len_1_itemsets, dataset_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def generate_candidates(freq_k_itemsets, k):\n",
    "    '''\n",
    "    Function to generate candidate k+1 itemsets from frequent k itemsets\n",
    "    merge two frequent itemsets if their first k - 1 items are identical (in order)\n",
    "    -------------\n",
    "    EXAMPLE:\n",
    "    ABC, ABD --> ABCD (k = 3)\n",
    "    '''\n",
    "    # candidate_itemsets = []\n",
    "     \n",
    "    candidate_itemsets = set()\n",
    "    freq_k_itemsets = [list(t) for t in freq_k_itemsets]\n",
    "    \n",
    "    for i, itemset_1 in enumerate(freq_k_itemsets[:-1]):\n",
    "        for itemset_2 in freq_k_itemsets[i+1:]:\n",
    "            itemset_1_k = itemset_1[:-1]\n",
    "            itemset_1_k.sort()\n",
    "            itemset_2_k = itemset_2[:-1]\n",
    "            itemset_2_k.sort()\n",
    "\n",
    "            if np.array_equal(itemset_1_k, itemset_2_k):\n",
    "                candidate = np.concatenate([itemset_1, [itemset_2[-1]]])\n",
    "                candidate_itemsets.add(tuple(candidate))\n",
    "\n",
    "    candidate_itemsets = [np.array(itemset) for itemset in candidate_itemsets]\n",
    "\n",
    "    print(f\"generated {len(candidate_itemsets)} candidates for {k+1}-itemsets\")\n",
    "    return candidate_itemsets\n",
    "    # return np.unique(np.array(candidate_itemsets), axis=0)\n",
    "\n",
    "def prune_candidates(candidate_itemsets, freq_k_itemsets, k):\n",
    "    '''\n",
    "    Candidate_itemsets are of length k+1\n",
    "    Function to prune candidates containing subsets of length k that are infrequent \n",
    "    -------------------\n",
    "    INPUTS\n",
    "        candidate_itemsets : np.array of lists \n",
    "    '''\n",
    "    if len(candidate_itemsets)==0:\n",
    "        return candidate_itemsets\n",
    "    if k == 1:\n",
    "        # nothing to prune, candidate-2-itemsets are all made up of frequent-1-itemsets\n",
    "        print(f\"k==1, returning candidates as it is.\")\n",
    "        return candidate_itemsets\n",
    "    \n",
    "    pruned_itemsets = candidate_itemsets.copy()\n",
    "    freq_k_itemsets = [tuple(sorted(t)) for t in freq_k_itemsets]\n",
    "    idx_to_prune = []\n",
    "    \n",
    "    for idx, itemset in enumerate(pruned_itemsets):\n",
    "        itemset = sorted(itemset)\n",
    "        for subset in combinations(itemset, k):\n",
    "            subset = tuple(sorted(subset))\n",
    "            if subset not in freq_k_itemsets: \n",
    "                idx_to_prune.append(idx)\n",
    "                # print(f\"{subset} is not frequent. Breaking loop\")\n",
    "                break # don't need to further check, pruning this candidate itemset \n",
    "            # else : \n",
    "                # print(f\"{subset} is frequent\")\n",
    "        # if idx not in idx_to_prune:\n",
    "        #     print(f\"{itemset} survived all checks!\")\n",
    "    \n",
    "    pruned_itemsets = [pruned_itemsets[i] for i in range(len(pruned_itemsets)) if i not in idx_to_prune]\n",
    "    results = np.unique(np.array(pruned_itemsets), axis=0)\n",
    "    print(f\"pruned candidates from candidate {k+1} itemsets, left with {len(results)}\")\n",
    "    return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def chunk__count_support(chunk_idx, candidate_itemsets, chunk=None):\n",
    "    itemset_support = {idx: 0 for idx, _ in enumerate(candidate_itemsets)}\n",
    "    \n",
    "    start = time.time()\n",
    "    list_of_lists = get_list_of_lists(chunk_idx, chunk)\n",
    "\n",
    "    if chunk_idx%10==0:\n",
    "        print (f'chunk {chunk_idx}:\\t {round(time.time() - start,2)}')\n",
    "    \n",
    "    candidate_itemsets = [frozenset(itemset) for itemset in candidate_itemsets]\n",
    "    itemset_counter = Counter(frozenset(itemset) for transaction in list_of_lists \n",
    "                              for itemset in candidate_itemsets \n",
    "                              if frozenset(itemset).issubset(set(transaction)))\n",
    "\n",
    "    for idx, itemset in enumerate(candidate_itemsets):\n",
    "        itemset_support[idx] += itemset_counter[frozenset(itemset)]\n",
    "\n",
    "    # count = 0 \n",
    "    # for transaction in list_of_lists:\n",
    "    #     if count%10_000==0:\n",
    "    #         print(f\"cleared {count} transactions...\")\n",
    "    #     for idx, itemset in enumerate(candidate_itemsets):\n",
    "    #         if frozenset(itemset).issubset(frozenset(transaction)):\n",
    "    #             itemset_support[idx] +=1\n",
    "    #     count+=1\n",
    "                \n",
    "    return itemset_support\n",
    "                    \n",
    "def count_support(candidate_itemsets, chunk_iter_len, k, chunk_iter = None):\n",
    "    '''\n",
    "    scans through the entire database in chunks and counts the support\n",
    "    for every itemset in cand\n",
    "    idate_itemsets\n",
    "    \n",
    "    must check that all items in the itemset appear in the transaction \n",
    "    '''\n",
    "    if len(candidate_itemsets)==0:\n",
    "        return {}\n",
    "    \n",
    "    print(f\"counting support for {len(candidate_itemsets)} candidate {k+1} itemset\")\n",
    "    num_threads = multiprocessing.cpu_count()\n",
    "    if chunk_iter is None:\n",
    "        chunk_iter = [chunk_iter for i in range(chunk_iter_len)] # list of Nones\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "\n",
    "        results = list(executor.map(chunk__count_support,\n",
    "                                    range(chunk_iter_len), \n",
    "                                    [candidate_itemsets for _ in range(chunk_iter_len)], \n",
    "                                    chunk_iter\n",
    "                                    ), \n",
    "                        )\n",
    "    combined_dict = combine_dictionaries(results)\n",
    "    \n",
    "    return combined_dict\n",
    "    \n",
    "    # indexes = np.arange(len(candidate_itemsets))\n",
    "    # inital_counts = np.zeros(len(candidate_itemsets))\n",
    "    # itemset_support = dict(zip(indexes, inital_counts))\n",
    "    \n",
    "    # print('-------------------preprocessing reviews------------------')\n",
    "    # for chunk_idx, chunk in enumerate(data_chunks):\n",
    "    #     start = time.time()\n",
    "    #     list_of_lists = Parallel(n_jobs=4)(delayed(preprocess_str)(line) for line in chunk['text'])\n",
    "    #     # print (f'chunk {chunk_idx+1}:\\t', round(time.time() - start,2))\n",
    "        \n",
    "    #     # list_of_lists = [row.dropna().tolist() for index, row in chunk.iterrows()]\n",
    "    \n",
    "    #     for transaction in list_of_lists:\n",
    "            \n",
    "    #         for idx, itemset in enumerate(candidate_itemsets):\n",
    "    #             all_appear = set(itemset).issubset(set(transaction))\n",
    "    #             if all_appear:\n",
    "    #                 # update the support \n",
    "    #                 itemset_support[idx] +=1\n",
    "                    \n",
    "    # return itemset_support\n",
    "\n",
    "def get_frequent(candidate_itemsets, itemset_support, minsup, k):\n",
    "    freq_k_plus_itemsets = {tuple(candidate_itemsets[i]) : count for i, count in itemset_support.items() if count>minsup}\n",
    "    print(f\"generated {len(freq_k_plus_itemsets)} freq-{k+1}-itemsets from {len(candidate_itemsets)} candidate {k} itemsets\")\n",
    "    return list(freq_k_plus_itemsets.keys()), freq_k_plus_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori(minsup_frac, num_transactions=None, chunk_iter_len=None):\n",
    "    start = time.time()\n",
    "    num_freq = {}\n",
    "    results_dict = []\n",
    "    \n",
    "    if num_transactions is None:\n",
    "        chunk_iter = get_chunk_iter_processed()\n",
    "        chunk_iter_len = 0 \n",
    "        num_transactions = 0 \n",
    "        for chunk in chunk_iter:\n",
    "            chunk_iter_len+=1\n",
    "            num_transactions += len(chunk)\n",
    "        chunk_iter = get_chunk_iter_processed()\n",
    "        read_separately = False\n",
    "    else:\n",
    "        read_separately = True\n",
    "        chunk_iter = None\n",
    "        \n",
    "    minsup = minsup_frac*num_transactions\n",
    "    print(f'minsup selected: {minsup} ({minsup_frac}*{num_transactions} rows)')\n",
    "    len_1_itemsets = get_all_unique_items(chunk_iter_len, chunk_iter)\n",
    "    freq_k_dict = {item:count for item, count in len_1_itemsets.items() if count>minsup}\n",
    "    \n",
    "    freq_k_itemsets = np.array([[item] for item, count in len_1_itemsets.items() if count>minsup])\n",
    "    print(f\"generated {len(freq_k_itemsets)} freq-1-itemsets\")\n",
    "    \n",
    "    num_freq[1] = len(freq_k_itemsets)\n",
    "    results_dict.append(freq_k_dict)\n",
    "    k = 1\n",
    "    \n",
    "    while(len(freq_k_itemsets)>0):\n",
    "        candidates = generate_candidates(freq_k_itemsets, k)\n",
    "        pruned = prune_candidates(candidates, freq_k_itemsets, k)\n",
    "    \n",
    "        if not read_separately :\n",
    "            chunk_iter = get_chunk_iter_processed()\n",
    "            \n",
    "        itemset_support = count_support(pruned,chunk_iter_len, k, chunk_iter)\n",
    "        \n",
    "        freq_k_plus_itemsets, freq_k_plus_dict = get_frequent(pruned, itemset_support, minsup, k)\n",
    "        results_dict.append(freq_k_plus_dict)\n",
    "        num_freq[k+1] = len(freq_k_plus_itemsets)\n",
    "        \n",
    "        if len(freq_k_plus_itemsets) == 0 :\n",
    "            end = time.time()\n",
    "            time_taken = end-start\n",
    "            return time_taken, num_freq, results_dict\n",
    "        \n",
    "        freq_k_itemsets = freq_k_plus_itemsets\n",
    "        freq_k_dict = freq_k_plus_dict\n",
    "        k+=1\n",
    "    end = time.time()\n",
    "    time_taken = end-start\n",
    "    return time_taken, num_freq, results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minsup selected: 694269.4 (0.1*6942694 rows)\n",
      "------------------FIRST RUN------------------\n",
      "21843 unique items found in chunk 0. Took 1.48\n",
      "22183 unique items found in chunk 10. Took 1.15\n",
      "21914 unique items found in chunk 20. Took 1.33\n",
      "22505 unique items found in chunk 30. Took 1.25\n",
      "22066 unique items found in chunk 40. Took 1.23\n",
      "21745 unique items found in chunk 50. Took 1.35\n",
      "21675 unique items found in chunk 60. Took 1.71\n",
      "21749 unique items found in chunk 70. Took 1.11\n",
      "21600 unique items found in chunk 80. Took 1.11\n",
      "21871 unique items found in chunk 90. Took 1.22\n",
      "384302 unique items\n",
      "generated 3 freq-1-itemsets\n",
      "generated 3 candidates for 2-itemsets\n",
      "k==1, returning candidates as it is.\n",
      "counting support for 3 candidate 2 itemset\n",
      "chunk 0:\t 5.18\n",
      "chunk 10:\t 9.26\n",
      "chunk 20:\t 9.6\n",
      "chunk 30:\t 9.09\n",
      "chunk 40:\t 9.05\n",
      "chunk 50:\t 9.25\n",
      "chunk 60:\t 9.45\n",
      "chunk 70:\t 9.63\n",
      "chunk 80:\t 11.37\n",
      "chunk 90:\t 8.85\n",
      "3 unique items\n",
      "generated 0 freq-2-itemsets from 3 candidate 1 itemsets\n"
     ]
    }
   ],
   "source": [
    "time_taken, num_freq, results_dict = apriori(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
